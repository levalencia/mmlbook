{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (2.7.0.dev20250120)\n",
      "Requirement already satisfied: filelock in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/luisvalencia/Desktop/PythonPersonalProjects/mmlbook/.venv/lib/python3.13/site-packages (from jinja2->torch) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-Item Cosine Similarity:\n",
      " tensor([[1.0000, 0.8800, 0.0600, 0.3000, 0.0000],\n",
      "        [0.8800, 1.0000, 0.2200, 0.1400, 0.0000],\n",
      "        [0.0600, 0.2200, 1.0000, 0.7500, 0.8100],\n",
      "        [0.3000, 0.1400, 0.7500, 1.0000, 0.6200],\n",
      "        [0.0000, 0.0000, 0.8100, 0.6200, 1.0000]])\n",
      "Most similar item to Item 0 is Item 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Create a sample user-item rating matrix (rows = users, cols = items)\n",
    "# Ratings are from 1-5, 0 indicates no rating\n",
    "\n",
    "# Compute item-item cosine similarity matrix\n",
    "# Formula: sim(i,j) = (ratings[:,i] · ratings[:,j]) / (||ratings[:,i]|| * ||ratings[:,j]||)\n",
    "\n",
    "# Step 1: Calculate L2 norms of each item vector\n",
    "# Step 2: Compute dot products between all pairs of items\n",
    "# Step 3: Calculate denominator (outer product of norms) with epsilon to avoid division by zero\n",
    "# Step 4: Compute final similarity matrix\n",
    "\n",
    "# Example rating matrix: rows = users, cols = items\n",
    "# 0 indicates no rating (unknown)\n",
    "ratings = torch.tensor([\n",
    "    [5.0, 4.0, 0.0, 1.0, 0.0],   # User 0\n",
    "    [0.0, 2.0, 3.0, 0.0, 0.0],   # User 1\n",
    "    [1.0, 0.0, 2.0, 5.0, 0.0],   # User 2\n",
    "    [0.0, 0.0, 5.0, 4.0, 3.0]    # User 3\n",
    "])\n",
    "\n",
    "num_users, num_items = ratings.shape\n",
    "\n",
    "# Compute item-item cosine similarity matrix\n",
    "# Cosine sim(i,j) = (ratings[:,i] · ratings[:,j]) / (||ratings[:,i]|| * ||ratings[:,j]||)\n",
    "# First, compute norms of each item vector\n",
    "item_norms = torch.norm(ratings, dim=0)  # norm for each column (item)\n",
    "# Compute the dot product between every pair of item columns\n",
    "# ratings^T * ratings gives matrix of dot products between item vectors\n",
    "dot_products = ratings.T @ ratings  # shape (num_items, num_items)\n",
    "# Outer product of norms to get denominator matrix (and add a tiny epsilon to avoid division by zero)\n",
    "denom = item_norms.unsqueeze(1) * item_norms.unsqueeze(0) + 1e-8\n",
    "sim_matrix = dot_products / denom\n",
    "\n",
    "# Print similarity matrix (rounded for readability)\n",
    "print(\"Item-Item Cosine Similarity:\\n\", sim_matrix.round(decimals=2))\n",
    "# Let's find the item most similar to item 0\n",
    "item0_similarities = sim_matrix[0]\n",
    "# Exclude item 0 itself by setting its similarity to -inf temporarily\n",
    "item0_similarities[0] = -1.0  \n",
    "most_similar_item = torch.argmax(item0_similarities).item()\n",
    "print(f\"Most similar item to Item 0 is Item {most_similar_item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: MSE = 12.3051\n",
      "Epoch 100: MSE = 0.4406\n",
      "Epoch 200: MSE = 0.3193\n",
      "Epoch 300: MSE = 0.0109\n",
      "Epoch 400: MSE = 0.0000\n",
      "Epoch 500: MSE = 0.0000\n",
      "\n",
      "Learned user embeddings:\n",
      "tensor([[ 1.1778, -1.1229,  0.9697],\n",
      "        [-0.5731,  0.4176,  1.1482],\n",
      "        [-1.2671, -0.3199,  1.4713],\n",
      "        [-0.9060,  1.4882,  0.5722]])\n",
      "\n",
      "Learned item embeddings:\n",
      "tensor([[ 1.4137, -0.9943,  0.8733],\n",
      "        [ 0.5803, -0.8284,  0.9683],\n",
      "        [-0.8274,  1.8766,  0.3475],\n",
      "        [-1.2905,  0.3359,  1.5119],\n",
      "        [-0.5835,  0.6339,  0.4593]])\n",
      "\n",
      "Top 2 recommended items for user 0: [4, 2]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the training data: list of (user_id, item_id, rating) tuples\n",
    "# Each tuple represents a known rating from a user for an item\n",
    "ratings = [\n",
    "    (0, 0, 5.0), (0, 1, 4.0), (0, 3, 1.0),  # User 0's ratings\n",
    "    (1, 1, 2.0), (1, 2, 3.0),               # User 1's ratings\n",
    "    (2, 0, 1.0), (2, 2, 2.0), (2, 3, 5.0),  # User 2's ratings\n",
    "    (3, 2, 5.0), (3, 3, 4.0), (3, 4, 3.0)   # User 3's ratings\n",
    "]\n",
    "\n",
    "# Define the dimensions of our rating matrix\n",
    "num_users = 4   # users 0 through 3\n",
    "num_items = 5   # items 0 through 4\n",
    "\n",
    "# Define the number of latent factors (K)\n",
    "# This is a hyperparameter that determines the dimensionality of our latent space\n",
    "K = 3  # small number of latent features\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for collaborative filtering.\n",
    "    \n",
    "    This model learns:\n",
    "    - User embeddings: Each user is represented by a K-dimensional vector\n",
    "    - Item embeddings: Each item is represented by a K-dimensional vector\n",
    "    - User biases: Captures user rating tendencies\n",
    "    - Item biases: Captures item rating tendencies\n",
    "    \n",
    "    The predicted rating is computed as: dot_product(user_embedding, item_embedding) + user_bias + item_bias\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_items, num_factors):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        # Create embedding layers for users and items\n",
    "        self.user_emb = nn.Embedding(num_users, num_factors)\n",
    "        self.item_emb = nn.Embedding(num_items, num_factors)\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        # This helps with training stability\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_emb.weight, std=0.1)\n",
    "        \n",
    "        # Create bias terms for users and items\n",
    "        # These capture individual rating tendencies\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            user_ids: Tensor of user IDs\n",
    "            item_ids: Tensor of item IDs\n",
    "            \n",
    "        Returns:\n",
    "            Predicted ratings for the given user-item pairs\n",
    "        \"\"\"\n",
    "        # Get embedding vectors for users and items\n",
    "        u_vecs = self.user_emb(user_ids)        # shape: [batch_size, K]\n",
    "        i_vecs = self.item_emb(item_ids)        # shape: [batch_size, K]\n",
    "        \n",
    "        # Compute dot product for each user-item pair\n",
    "        dot = (u_vecs * i_vecs).sum(dim=1)      # shape: [batch_size]\n",
    "        \n",
    "        # Add user and item biases\n",
    "        u_bias = self.user_bias(user_ids).squeeze()  # shape: [batch_size]\n",
    "        i_bias = self.item_bias(item_ids).squeeze()  # shape: [batch_size]\n",
    "        \n",
    "        return dot + u_bias + i_bias\n",
    "\n",
    "# Create model instance and training components\n",
    "model = MatrixFactorization(num_users, num_items, K)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer with learning rate 0.01\n",
    "\n",
    "# Prepare training data as tensors\n",
    "user_tensor = torch.tensor([u for u, i, r in ratings])\n",
    "item_tensor = torch.tensor([i for u, i, r in ratings])\n",
    "rating_tensor = torch.tensor([r for u, i, r in ratings])\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "model.train()  # Set model to training mode\n",
    "for epoch in range(501):\n",
    "    # Forward pass: predict ratings for all known pairs\n",
    "    preds = model(user_tensor, item_tensor)\n",
    "    loss = loss_fn(preds, rating_tensor)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update parameters\n",
    "    \n",
    "    # Print progress every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: MSE = {loss.item():.4f}\")\n",
    "\n",
    "# Display learned embeddings\n",
    "print(\"\\nLearned user embeddings:\")\n",
    "print(model.user_emb.weight.data)\n",
    "print(\"\\nLearned item embeddings:\")\n",
    "print(model.item_emb.weight.data)\n",
    "\n",
    "# Generate recommendations for a specific user\n",
    "def get_recommendations(user_id, k=2):\n",
    "    \"\"\"\n",
    "    Generate top-k recommendations for a user.\n",
    "    \n",
    "    Args:\n",
    "        user_id: ID of the user to generate recommendations for\n",
    "        k: Number of recommendations to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended item IDs\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Create tensors for all possible items for this user\n",
    "    user_ids = torch.tensor([user_id] * num_items)\n",
    "    item_ids = torch.arange(num_items)\n",
    "    \n",
    "    # Get predicted ratings for all items\n",
    "    pred_scores = model(user_ids, item_ids)\n",
    "    \n",
    "    # Mask items the user has already rated\n",
    "    seen_items = {i for (u, i, r) in ratings if u == user_id}\n",
    "    for i in seen_items:\n",
    "        pred_scores[i] = -1e5  # Set very low score for seen items\n",
    "    \n",
    "    # Get top-k recommended items\n",
    "    recommended_ids = torch.topk(pred_scores, k=k).indices.tolist()\n",
    "    return recommended_ids\n",
    "\n",
    "# Example: Get recommendations for user 0\n",
    "user_id = 0\n",
    "recommendations = get_recommendations(user_id, k=2)\n",
    "print(f\"\\nTop 2 recommended items for user {user_id}: {recommendations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User profile vector: [0.5, 1.0, 0.5]\n",
      "Cosine similarity of each item to the user profile: [0.8660253882408142, 0.5773502588272095, 0.8660253882408142, 0.942808985710144, 0.8164965510368347]\n",
      "Top recommended item for the user (content-based): Item 3\n"
     ]
    }
   ],
   "source": [
    "# Example: 5 items, each with 3 content features (could be genre flags, for instance)\n",
    "item_features = torch.tensor([\n",
    "    [1.0, 1.0, 0.0],  # Item 0: perhaps this means it has Feature0 and Feature1\n",
    "    [1.0, 0.0, 1.0],  # Item 1: has Feature0 and Feature2\n",
    "    [0.0, 1.0, 1.0],  # Item 2: has Feature1 and Feature2\n",
    "    [1.0, 1.0, 1.0],  # Item 3: has all three features\n",
    "    [0.0, 1.0, 0.0]   # Item 4: has only Feature1\n",
    "])\n",
    "\n",
    "# Let's say our user has liked item 0 and item 2 in the past.\n",
    "liked_items = [0, 2]\n",
    "\n",
    "# Build the user profile by averaging the feature vectors of liked items (one simple strategy).\n",
    "user_profile = item_features[liked_items].mean(dim=0)\n",
    "print(\"User profile vector:\", user_profile.tolist())\n",
    "\n",
    "# Compute cosine similarity between the user profile and each item's features\n",
    "norm_user = torch.norm(user_profile)\n",
    "item_norms = torch.norm(item_features, dim=1)\n",
    "# Compute dot product between user profile and each item feature vector\n",
    "dot_scores = item_features @ user_profile  # shape: (num_items,)\n",
    "cosine_sim = dot_scores / (item_norms * norm_user + 1e-8)\n",
    "\n",
    "print(\"Cosine similarity of each item to the user profile:\", cosine_sim.tolist())\n",
    "\n",
    "# Exclude the items the user already liked\n",
    "mask = torch.ones(item_features.size(0), dtype=torch.bool)\n",
    "mask[liked_items] = False\n",
    "unseen_similarity = cosine_sim * mask  # zero out seen items\n",
    "\n",
    "# Recommend the top item based on similarity\n",
    "recommended_item = torch.argmax(unseen_similarity).item()\n",
    "print(f\"Top recommended item for the user (content-based): Item {recommended_item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 15.3861\n",
      "Epoch 100: MSE = 1.7804\n",
      "Epoch 200: MSE = 1.7712\n",
      "Epoch 300: MSE = 1.7652\n",
      "Epoch 400: MSE = 1.7614\n",
      "Epoch 500: MSE = 1.7589\n",
      "Predicted rating for User 3 on Item 0: 3.76\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume we have `ratings` list and `item_features` tensor defined from before.\n",
    "# (ratings: list of (user, item, rating), item_features: tensor of shape [num_items, num_features])\n",
    "\n",
    "num_features = item_features.size(1)\n",
    "hidden_dim = 8  # number of hidden units for our neural network\n",
    "\n",
    "class HybridRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_item_features, hidden_dim):\n",
    "        super(HybridRecommender, self).__init__()\n",
    "        # Embedding for user IDs (collaborative part)\n",
    "        self.user_emb = nn.Embedding(num_users, hidden_dim)\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.1)\n",
    "        # A simple feed-forward network for combining user embedding with item features\n",
    "        self.fc1 = nn.Linear(num_item_features + hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, user_ids, item_feats):\n",
    "        # user_ids: tensor of shape [batch_size]\n",
    "        # item_feats: tensor of shape [batch_size, num_item_features]\n",
    "        u_vecs = self.user_emb(user_ids)               # [batch_size, hidden_dim]\n",
    "        x = torch.cat([u_vecs, item_feats], dim=1)     # combine user embedding and item features\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)                                # output a single score (rating)\n",
    "        return x.squeeze()  # return 1D tensor (batch of scores)\n",
    "\n",
    "# Instantiate model\n",
    "hybrid_model = HybridRecommender(num_users, num_features, hidden_dim)\n",
    "optimizer = optim.Adam(hybrid_model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Prepare data (same ratings list, but now need item feature for each example)\n",
    "user_ids = torch.tensor([u for u, i, r in ratings])\n",
    "item_ids = [i for u, i, r in ratings]\n",
    "item_feats_tensor = torch.stack([item_features[i] for i in item_ids])\n",
    "ratings_tensor = torch.tensor([r for u, i, r in ratings])\n",
    "\n",
    "# Training loop for the hybrid model\n",
    "hybrid_model.train()\n",
    "for epoch in range(501):\n",
    "    pred = hybrid_model(user_ids, item_feats_tensor)\n",
    "    loss = loss_fn(pred, ratings_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: MSE = {loss.item():.4f}\")\n",
    "\n",
    "# Let's use the trained hybrid model to predict user 3's rating for item 0 (as an example)\n",
    "hybrid_model.eval()\n",
    "test_user = torch.tensor([3])\n",
    "test_item_feat = item_features[0].unsqueeze(0)  # features for item 0\n",
    "predicted_rating = hybrid_model(test_user, test_item_feat).item()\n",
    "print(f\"Predicted rating for User 3 on Item 0: {predicted_rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 11.3887\n",
      "Epoch 100: MSE = 1.6547\n",
      "Epoch 200: MSE = 1.6515\n",
      "Epoch 300: MSE = 1.6515\n",
      "Epoch 400: MSE = 1.6515\n",
      "Epoch 500: MSE = 1.6515\n",
      "Neural CF predicted score for User 0 on Item 2: 3.500\n"
     ]
    }
   ],
   "source": [
    "class NeuralCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_dim, hidden_dims):\n",
    "        super(NeuralCF, self).__init__()\n",
    "        # Embedding layers for user and item\n",
    "        self.user_emb = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, embed_dim)\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_emb.weight, std=0.1)\n",
    "        # Fully connected layers for the MLP part\n",
    "        # We'll create a sequential model from the list of hidden_dims\n",
    "        layers = []\n",
    "        input_dim = embed_dim * 2  # since we will concatenate user and item embedding\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h\n",
    "        layers.append(nn.Linear(input_dim, 1))  # final output layer\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        u = self.user_emb(user_ids)\n",
    "        v = self.item_emb(item_ids)\n",
    "        # Concatenate user and item embedding vectors\n",
    "        x = torch.cat([u, v], dim=1)\n",
    "        # Pass through MLP\n",
    "        out = self.mlp(x)\n",
    "        return out.squeeze()  # return a scalar prediction\n",
    "\n",
    "# Instantiate the neural CF model\n",
    "ncf_model = NeuralCF(num_users, num_items, embed_dim=5, hidden_dims=[20, 10])\n",
    "optimizer = optim.Adam(ncf_model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "user_tensor = torch.tensor([u for u, i, r in ratings])\n",
    "item_tensor = torch.tensor([i for u, i, r in ratings])\n",
    "rating_tensor = torch.tensor([r for u, i, r in ratings])\n",
    "\n",
    "# Training loop\n",
    "ncf_model.train()\n",
    "for epoch in range(501):\n",
    "    pred = ncf_model(user_tensor, item_tensor)\n",
    "    loss = loss_fn(pred, rating_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: MSE = {loss.item():.4f}\")\n",
    "\n",
    "# Example prediction: user 0 on item 2 (which user 0 has not rated in our data)\n",
    "ncf_model.eval()\n",
    "test_user = torch.tensor([0])\n",
    "test_item = torch.tensor([2])\n",
    "score = ncf_model(test_user, test_item).item()\n",
    "print(f\"Neural CF predicted score for User 0 on Item 2: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
