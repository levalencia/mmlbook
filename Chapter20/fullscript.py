#!/usr/bin/env python3
"""
Deploying Models ‚Äì End-to-End Pipeline
=====================================
This script accompanies *Chapter XX ‚Äì Deploying Models* of the book and
shows the complete journey **after** a model has been trained:

    0.  Conceptual foundations  ‚Ä¶‚Ä¶  (read below)
    1.  Environment & dependencies ‚Ä¶  `setup_environment()`
    2.  Data ingestion/preparation ‚Ä¶  `load_mnist_data()`
    3.  Training & evaluation     ‚Ä¶  `train_model()` / `evaluate_model()`
    4.  Persistence               ‚Ä¶  `save_models()` / `load_models()`
    5.  Optimisation (compression) ‚Ä¶  `quantize_model()` / `prune_model()`
    6.  Inter-operability formats ‚Ä¶  `convert_to_torchscript()` / `convert_to_onnx()`
    7.  Versioning                ‚Ä¶  `ModelVersionManager`
    8.  Packaging & deployment    ‚Ä¶  `deploy_to_azure()`
    9.  Online inference entrypoint ‚Ä¶  `score.py` generated by `create_deployment_files()`

Key Concepts Explained
----------------------
‚Ä¢ *Scoring* vs *Inference*: In Azure ML an **entry script** (`score.py`) is
  executed by the inference server. When it receives an HTTP request it
  *scores* (evaluates) the input, i.e. performs forward inference.  In
  everyday speech scoring == inference ‚Äì Azure just keeps the older
  term.

‚Ä¢ *State-dict* (`*.pth`) vs *full model pickle*: Saving `model.state_dict()`
  stores only the tensor weights ‚Üí portable & version-agnostic.  Pickling
  the entire `nn.Module` captures code & weights but is brittle (Python
  version, local class names).  Therefore we **always** checkpoint the
  state-dict and pickle **only** when the class is globally defined.

‚Ä¢ *TorchScript* (`*.pt`):  PyTorch‚Äôs ahead-of-time format created via
  `torch.jit.trace` or `torch.jit.script`.  It runs in the lightweight
  C++ runtime (no Python) ‚Äì great for mobile or production services.

‚Ä¢ *ONNX* (`*.onnx`):  Open standard for model interchange.  Once exported,
  the model can be executed by any ONNX-Runtime compatible engine and even
  hardware accelerators.

‚Ä¢ *Quantisation* & *Pruning*:  Post-training techniques to shrink model
  size or speed up inference.  Both are optional but demonstrate how to
  chain optimisation passes **before** deployment.

‚Ä¢ *Azure ML Environments*: A managed wrapper around a Conda + Docker image.
  We define ours in `environment.yml`; Azure builds a Docker image, pushes
  it to Azure Container Registry (ACR) and deploys it to the endpoint.

‚Ä¢ *Managed Online Endpoint* vs *Deployment*: An *endpoint* is the stable
  HTTPS URL.  Each *deployment* is a version of the model + environment
  (blue/green, canary etc.).  Traffic percentages route requests.

‚Ä¢ *Model Versioning*:  Locally via `ModelVersionManager`; in Azure ML via
  the `Model` registry.  Both use semantic timestamps + content hashes.

Navigate the File
-----------------
Search for
    `# ===== SECTION`  delimiters to follow the execution path.
Each function also contains verbose doc-strings focused on *why* ‚Äì not
just *what* ‚Äì it is doing.
"""

import os
import sys
import json
import time
import datetime
import hashlib
import tempfile
import logging
import traceback
from pathlib import Path

# Configure comprehensive logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('model_pipeline.log', mode='w')
    ]
)
logger = logging.getLogger(__name__)

# ===== SECTION 1 ‚Äì ENVIRONMENT & DEPENDENCIES ==============================
# Everything required to import libraries, create working folders and check
# versions lives in this block.  Readers should understand how Conda, a
# YAML environment file and ultimately a Docker image combine into the
# Azure ML *environment* concept.
# ==========================================================================

def setup_environment():
    """Set up the environment and imports"""
    logger.info("=== SETTING UP ENVIRONMENT ===")
    
    try:
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torchvision import datasets, transforms
        from torch.utils.data import DataLoader
        import torch.quantization as quantization
        import torch.nn.utils.prune as prune
        import torch.jit as jit
        import torch.onnx as onnx
        
        logger.info(f"‚úì PyTorch imported successfully - Version: {torch.__version__}")
        
        # Create necessary directories
        os.makedirs('models', exist_ok=True)
        os.makedirs('model_versions', exist_ok=True)
        os.makedirs('data', exist_ok=True)
        logger.info("‚úì Directories created")
        
        return torch, nn, optim, datasets, transforms, DataLoader, quantization, prune, jit, onnx
        
    except ImportError as e:
        logger.error(f"‚ùå Failed to import required packages: {e}")
        logger.info("Please install required packages: pip install torch torchvision")
        sys.exit(1)

def load_mnist_data(datasets, transforms, DataLoader):
    """Load and prepare MNIST dataset"""
    logger.info("=== LOADING MNIST DATASET ===")
    
    try:
        transform = transforms.Compose([transforms.ToTensor()])
        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
        test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
        
        logger.info(f"‚úì Training samples: {len(train_dataset)}")
        logger.info(f"‚úì Test samples: {len(test_dataset)}")
        
        return train_loader, test_loader
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load MNIST data: {e}")
        raise

# Global SimpleNN class - will be set after torch import
SimpleNN = None

def create_simple_nn_class(torch, nn):
    """Create SimpleNN class with proper torch dependencies"""
    global SimpleNN
    
    class SimpleNNClass(nn.Module):
        """Simple Neural Network for MNIST classification"""
        
        def __init__(self, input_size=784, hidden_size=128, num_classes=10):
            super(SimpleNNClass, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, num_classes)
            logger.info(f"‚úì SimpleNN initialized: {input_size} -> {hidden_size} -> {num_classes}")

        def forward(self, x):
            x = x.view(-1, 28*28)  # Flatten the image
            x = torch.relu(self.fc1(x))
            x = self.fc2(x)
            return x
    
    # Set global SimpleNN for pickling compatibility
    SimpleNN = SimpleNNClass
    return SimpleNNClass

def train_model(optimizer_name, train_loader, nn, optim, torch):
    """Train the neural network"""
    logger.info(f"=== TRAINING MODEL WITH {optimizer_name} ===")
    
    try:
        SimpleNNClass = create_simple_nn_class(torch, nn)
        model = SimpleNNClass()
        
        # Choose optimizer
        if optimizer_name == 'SGD':
            optimizer = optim.SGD(model.parameters(), lr=0.01)
        else:
            optimizer = optim.Adam(model.parameters(), lr=0.001)

        criterion = nn.CrossEntropyLoss()
        model.train()

        # Training loop
        for epoch in range(3):  # Train for 3 epochs
            epoch_loss = 0
            batch_count = 0
            
            for inputs, labels in train_loader:
                optimizer.zero_grad()  # Zero the gradients
                outputs = model(inputs)  # Forward pass
                loss = criterion(outputs, labels)  # Calculate loss
                loss.backward()  # Backpropagate
                optimizer.step()  # Update weights
                
                epoch_loss += loss.item()
                batch_count += 1
                
                if batch_count % 200 == 0:
                    logger.info(f"    Epoch {epoch+1}, Batch {batch_count}, Loss: {loss.item():.4f}")
            
            avg_loss = epoch_loss / batch_count
            logger.info(f"  Epoch {epoch+1} completed - Average Loss: {avg_loss:.4f}")

        logger.info(f"‚úì {optimizer_name} training completed")
        return model
        
    except Exception as e:
        logger.error(f"‚ùå Training failed: {e}")
        raise

def evaluate_model(model, test_loader, torch):
    """Evaluate the model's performance"""
    logger.info("=== EVALUATING MODEL ===")
    
    try:
        model.eval()  # Set the model to evaluation mode
        correct = 0
        total = 0
        
        with torch.no_grad():  # Disable gradient calculation
            for inputs, labels in test_loader:
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)  # Get the predicted class
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        accuracy = correct / total * 100
        logger.info(f"‚úì Model accuracy: {accuracy:.2f}%")
        return accuracy
        
    except Exception as e:
        logger.error(f"‚ùå Evaluation failed: {e}")
        raise

def save_models(model, torch):
    """Save models in different formats"""
    logger.info("=== SAVING MODELS ===")
    
    try:
        # Save state dictionary (Recommended)
        torch.save(model.state_dict(), 'models/simple_nn_state_dict.pth')
        logger.info("‚úì Model state dictionary saved")
        
        # Try to save entire model, but handle pickling issues gracefully
        try:
            torch.save(model, 'models/simple_nn_complete.pth')
            logger.info("‚úì Complete model saved")
        except Exception as pickle_error:
            logger.warning(f"‚ö†Ô∏è  Could not save complete model due to pickling issue: {pickle_error}")
            logger.info("‚ÑπÔ∏è  This is normal for dynamically created classes. State dict saved successfully.")
        
    except Exception as e:
        logger.error(f"‚ùå Model saving failed: {e}")
        raise

def load_models(torch, nn):
    """Load models from saved files"""
    logger.info("=== LOADING MODELS ===")
    
    try:
        # Create SimpleNN class
        SimpleNNClass = create_simple_nn_class(torch, nn)
        
        # Load from state dictionary
        loaded_model_state = SimpleNNClass()
        loaded_model_state.load_state_dict(torch.load('models/simple_nn_state_dict.pth', map_location='cpu', weights_only=True))
        loaded_model_state.eval()
        logger.info("‚úì Model loaded from state dictionary")
        
        # Try to load complete model if it exists
        loaded_model_complete = None
        try:
            if os.path.exists('models/simple_nn_complete.pth'):
                loaded_model_complete = torch.load('models/simple_nn_complete.pth', map_location='cpu', weights_only=False)
                loaded_model_complete.eval()
                logger.info("‚úì Complete model loaded")
            else:
                logger.info("‚ÑπÔ∏è  Complete model file not found, using state dict model only")
                loaded_model_complete = loaded_model_state  # Use the same model
        except Exception as load_error:
            logger.warning(f"‚ö†Ô∏è  Could not load complete model: {load_error}")
            logger.info("‚ÑπÔ∏è  Using state dict model instead")
            loaded_model_complete = loaded_model_state  # Fallback to state dict model
        
        return loaded_model_state, loaded_model_complete
        
    except Exception as e:
        logger.error(f"‚ùå Model loading failed: {e}")
        raise

def quantize_model(model, torch, quantization):
    """Apply quantization to reduce model size"""
    logger.info("=== QUANTIZING MODEL ===")
    
    try:
        # Set quantization backend
        torch.backends.quantized.engine = "qnnpack"
        
        # Create sample input for calibration
        sample_input = torch.randn(32, 1, 28, 28)
        
        # Set the model to evaluation mode
        model.eval()
        
        # Configure quantization
        model.qconfig = quantization.get_default_qconfig('qnnpack')
        
        # Prepare the model for quantization
        model_prepared = quantization.prepare(model, inplace=False)
        
        # Calibrate with sample data
        with torch.no_grad():
            model_prepared(sample_input)
        
        # Convert to quantized model
        model_quantized = quantization.convert(model_prepared, inplace=False)
        
        logger.info("‚úì Model quantization successful")
        
        # Compare sizes
        original_size = get_model_size(model)
        quantized_size = get_model_size(model_quantized)
        
        logger.info(f"  Original model size: {original_size:.2f} MB")
        logger.info(f"  Quantized model size: {quantized_size:.2f} MB")
        logger.info(f"  Size reduction: {((original_size - quantized_size) / original_size * 100):.1f}%")
        
        return model_quantized
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Quantization failed: {e}")
        logger.info("Quantization may not be supported on this platform")
        return None

def get_model_size(model):
    """Calculate model size in MB"""
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    return (param_size + buffer_size) / 1024 / 1024

def prune_model(model, prune, nn, torch):
    """Apply pruning to reduce model parameters"""
    logger.info("=== PRUNING MODEL ===")
    
    try:
        # Create a new instance of the same model class
        global SimpleNN
        if SimpleNN is not None:
            model_pruned = SimpleNN()
        else:
            # Fallback: create new instance using the model's class
            model_pruned = type(model)()
        model_pruned.load_state_dict(model.state_dict())
        
        # Apply pruning to linear layers
        for name, module in model_pruned.named_modules():
            if isinstance(module, nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=0.3)
                prune.remove(module, 'weight')
        
        original_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        pruned_params = sum(p.numel() for p in model_pruned.parameters() if p.requires_grad)
        
        logger.info(f"‚úì Pruning completed")
        logger.info(f"  Original parameters: {original_params:,}")
        logger.info(f"  Pruned parameters: {pruned_params:,}")
        logger.info(f"  Parameter reduction: {((original_params - pruned_params) / original_params * 100):.1f}%")
        
        return model_pruned
        
    except Exception as e:
        logger.error(f"‚ùå Pruning failed: {e}")
        raise

def convert_to_torchscript(model, torch, jit):
    """Convert model to TorchScript"""
    logger.info("=== CONVERTING TO TORCHSCRIPT ===")
    
    try:
        model.eval()
        sample_input = torch.randn(1, 1, 28, 28)
        
        with torch.no_grad():
            traced_model = jit.trace(model, sample_input)
        
        # Save TorchScript model
        traced_model.save('models/simple_nn_traced.pt')
        logger.info("‚úì TorchScript model saved")
        
        # Test the traced model
        loaded_traced_model = jit.load('models/simple_nn_traced.pt')
        loaded_traced_model.eval()
        
        with torch.no_grad():
            original_output = model(sample_input)
            traced_output = loaded_traced_model(sample_input)
            
            if torch.allclose(original_output, traced_output, atol=1e-6):
                logger.info("‚úì TorchScript model verification passed")
            else:
                logger.warning("‚ö†Ô∏è  TorchScript outputs don't match exactly")
        
        return traced_model
        
    except Exception as e:
        logger.error(f"‚ùå TorchScript conversion failed: {e}")
        raise

def convert_to_onnx(model, torch):
    """Convert model to ONNX format"""
    logger.info("=== CONVERTING TO ONNX ===")
    
    try:
        model.eval()
        sample_input = torch.randn(1, 1, 28, 28)
        
        # Export to ONNX
        torch.onnx.export(
            model,
            sample_input,
            'models/simple_nn.onnx',
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['mnist_image'],
            output_names=['digit_probabilities'],
            dynamic_axes={
                'mnist_image': {0: 'batch_size'},
                'digit_probabilities': {0: 'batch_size'}
            }
        )
        logger.info("‚úì ONNX model exported")
        
        # Verify ONNX model if possible
        try:
            import onnx
            import onnxruntime as ort
            
            onnx_model = onnx.load('models/simple_nn.onnx')
            onnx.checker.check_model(onnx_model)
            logger.info("‚úì ONNX model verification passed")
            
            # Test inference
            ort_session = ort.InferenceSession('models/simple_nn.onnx')
            input_data = sample_input.numpy()
            ort_inputs = {ort_session.get_inputs()[0].name: input_data}
            ort_outputs = ort_session.run(None, ort_inputs)
            
            with torch.no_grad():
                pytorch_output = model(sample_input)
            
            if torch.allclose(pytorch_output, torch.tensor(ort_outputs[0]), atol=1e-6):
                logger.info("‚úì ONNX inference verification passed")
            else:
                logger.warning("‚ö†Ô∏è  ONNX outputs don't match exactly")
                
        except ImportError:
            logger.info("üìã ONNX verification skipped (onnx/onnxruntime not installed)")
        
    except Exception as e:
        logger.error(f"‚ùå ONNX conversion failed: {e}")
        raise

class ModelVersionManager:
    """Model versioning system"""
    
    def __init__(self, base_path='model_versions'):
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
        logger.info(f"‚úì Version manager initialized: {base_path}")
    
    def save_versioned_model(self, model, model_name, metadata=None):
        """Save model with version information"""
        logger.info(f"=== SAVING VERSIONED MODEL: {model_name} ===")
        
        try:
            import torch  # Local import to avoid global dependency
            
            # Generate version timestamp
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Create model hash
            model_state = model.state_dict()
            model_bytes = str(model_state).encode('utf-8')
            model_hash = hashlib.md5(model_bytes).hexdigest()[:8]
            
            # Create version directory
            version_name = f"{model_name}_v{timestamp}_{model_hash}"
            version_dir = self.base_path / version_name
            version_dir.mkdir(exist_ok=True)
            
            # Save model
            model_path = version_dir / 'model.pth'
            torch.save(model.state_dict(), model_path)
            
            # Save metadata
            version_metadata = {
                'model_name': model_name,
                'version': timestamp,
                'hash': model_hash,
                'created_at': datetime.datetime.now().isoformat(),
                'model_path': str(model_path),
                'custom_metadata': metadata or {}
            }
            
            metadata_path = version_dir / 'metadata.json'
            with open(metadata_path, 'w') as f:
                json.dump(version_metadata, f, indent=2)
            
            logger.info(f"‚úì Model saved as version: {version_name}")
            return version_name, version_dir
            
        except Exception as e:
            logger.error(f"‚ùå Version saving failed: {e}")
            raise
    
    def list_versions(self, model_name=None):
        """List all available versions"""
        try:
            versions = []
            for version_dir in self.base_path.iterdir():
                if version_dir.is_dir():
                    if model_name is None or version_dir.name.startswith(model_name):
                        metadata_path = version_dir / 'metadata.json'
                        if metadata_path.exists():
                            with open(metadata_path, 'r') as f:
                                metadata = json.load(f)
                            versions.append(metadata)
            
            versions = sorted(versions, key=lambda x: x['created_at'], reverse=True)
            logger.info(f"‚úì Found {len(versions)} versions")
            return versions
            
        except Exception as e:
            logger.error(f"‚ùå Version listing failed: {e}")
            raise

def setup_azure_ml():
    """Set up Azure ML workspace connection"""
    logger.info("=== SETTING UP AZURE ML CONNECTION ===")
    
    try:
        from azureml.core import Workspace
        from azure.identity import DefaultAzureCredential
        from azure.ai.ml import MLClient
        
        # Configuration
        SUBSCRIPTION_ID = "287ddee1-542c-4ae3-9c98-ed8d88dd64bc"
        RESOURCE_GROUP = "pytorchbook"
        WORKSPACE_NAME = "pytorchbook"
        
        # Try to get workspace
        try:
            ws = Workspace.get(
                name=WORKSPACE_NAME,
                subscription_id=SUBSCRIPTION_ID,
                resource_group=RESOURCE_GROUP
            )
            ws.write_config(path=".", file_name="config.json")
            logger.info("‚úì Azure ML workspace connected (v1 SDK)")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Azure ML v1 SDK connection failed: {e}")
        
        # Set up v2 SDK client
        try:
            cred = DefaultAzureCredential(exclude_shared_token_cache_credential=True)
            ml_client = MLClient(cred, SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)
            logger.info("‚úì Azure ML v2 client created")
            return ml_client
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Azure ML v2 client creation failed: {e}")
            return None
            
    except ImportError:
        logger.warning("‚ö†Ô∏è  Azure ML SDK not installed - skipping Azure ML steps")
        logger.info("Install with: pip install azureml-core azure-ai-ml azure-identity")
        return None
    except Exception as e:
        logger.error(f"‚ùå Azure ML setup failed: {e}")
        return None

def register_model_azure(ml_client):
    """Register model in Azure ML"""
    if ml_client is None:
        logger.info("‚è≠Ô∏è  Skipping Azure ML model registration - no client available")
        return None
        
    logger.info("=== REGISTERING MODEL IN AZURE ML ===")
    
    try:
        from azureml.core import Workspace, Model
        
        # Connect to workspace
        ws = Workspace.from_config()
        
        # Register model
        model = Model.register(
            workspace=ws,
            model_name='SimpleNN-MNIST',
            model_path='models/simple_nn_state_dict.pth',
            description='Simple Neural Network for MNIST digit classification',
            tags={
                'framework': 'PyTorch',
                'dataset': 'MNIST',
                'accuracy': '0.95'
            },
            model_framework=Model.Framework.PYTORCH,
            model_framework_version='1.9.0'
        )
        
        logger.info(f"‚úì Model registered: {model.name}, Version: {model.version}")
        return model
        
    except Exception as e:
        logger.error(f"‚ùå Azure ML model registration failed: {e}")
        return None

def create_deployment_files():
    """Create necessary files for Azure ML deployment"""
    logger.info("=== CREATING DEPLOYMENT FILES ===")
    
    try:
        # Create environment specification
        environment_spec = """
name: pytorch-cpu-inference
channels:
  - conda-forge
dependencies:
  - python=3.10
  - pip
  - pip:
      # self-contained CPU wheels
      - torch==2.3.0+cpu
      - torchvision==0.18.0+cpu
      - --extra-index-url https://download.pytorch.org/whl/cpu
      # utilities
      - numpy
      - pillow
      - inference-schema[numpy-support]
      - azureml-defaults
"""
        
        with open('environment.yml', 'w') as f:
            f.write(environment_spec)
        logger.info("‚úì Environment specification created")
        
        # Create scoring script
        scoring_script = '''
import json
import os
import sys
import logging
import traceback

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def init():
    """Initialize the model for inference"""
    global model
    
    try:
        logger.info("Starting model initialization...")
        
        import torch
        import torch.nn as nn
        
        # Define model architecture
        class SimpleNN(nn.Module):
            def __init__(self, input_size=784, hidden_size=128, num_classes=10):
                super(SimpleNN, self).__init__()
                self.fc1 = nn.Linear(input_size, hidden_size)
                self.fc2 = nn.Linear(hidden_size, num_classes)
              
            def forward(self, x):
                x = x.view(x.size(0), -1)
                x = torch.relu(self.fc1(x))
                x = self.fc2(x)
                return x
        
        # Find model file
        model_path = None
        search_paths = [
            "/var/azureml-app/azureml-models/SimpleNN-MNIST/*/simple_nn_state_dict.pth",
            "models/simple_nn_state_dict.pth",
            "simple_nn_state_dict.pth"
        ]
        
        import glob
        for pattern in search_paths:
            matches = glob.glob(pattern)
            if matches:
                model_path = matches[0]
                break
        
        if not model_path:
            raise FileNotFoundError("Model file not found")
        
        # Load model
        model = SimpleNN()
        state_dict = torch.load(model_path, map_location='cpu', weights_only=True)
        model.load_state_dict(state_dict)
        model.eval()
        
        logger.info("Model initialization complete")
        
    except Exception as e:
        logger.error(f"Model initialization failed: {e}")
        raise

def run(raw_data):
    """Process inference requests"""
    try:
        import torch
        import numpy as np
        
        data = json.loads(raw_data)
        
        # Handle different input formats
        if 'data' in data:
            input_data = data['data']
            if isinstance(input_data, list) and len(input_data) == 784:
                input_tensor = torch.tensor(input_data, dtype=torch.float32).view(1, 1, 28, 28)
            else:
                input_tensor = torch.tensor(input_data, dtype=torch.float32)
                if len(input_tensor.shape) == 2:
                    input_tensor = input_tensor.unsqueeze(0).unsqueeze(0)
        else:
            # Default test input
            input_tensor = torch.randn(1, 1, 28, 28)
        
        # Perform inference
        with torch.no_grad():
            outputs = model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            predicted_class = torch.argmax(probabilities, dim=1)
        
        response = {
            'predicted_digit': int(predicted_class[0]),
            'confidence': float(torch.max(probabilities[0])),
            'probabilities': probabilities[0].tolist(),
            'status': 'success'
        }
        
        return json.dumps(response)
        
    except Exception as e:
        error_response = {
            'error': str(e),
            'status': 'error'
        }
        return json.dumps(error_response)
'''
        
        with open('score.py', 'w') as f:
            f.write(scoring_script)
        logger.info("‚úì Scoring script created")
        
    except Exception as e:
        logger.error(f"‚ùå Deployment files creation failed: {e}")
        raise

def deploy_to_azure(ml_client):
    """Deploy model to Azure ML managed endpoint"""
    if ml_client is None:
        logger.info("‚è≠Ô∏è  Skipping Azure ML deployment - no client available")
        return
        
    logger.info("=== DEPLOYING TO AZURE ML ===")
    
    try:
        from azure.ai.ml.entities import (
            ManagedOnlineEndpoint,
            ManagedOnlineDeployment,
            Model,
            Environment,
            CodeConfiguration
        )
        from azure.core.exceptions import ResourceNotFoundError
        
        MODEL_NAME = "SimpleNN-MNIST"
        ENDPOINT_NAME = f"{MODEL_NAME}-ep"
        DEPLOYMENT_NAME = "blue"
        
        # Register model
        model = ml_client.models.create_or_update(
            Model(
                name=MODEL_NAME,
                path="models/simple_nn_state_dict.pth",
                type="custom_model",
                description="Simple NN for MNIST digit classification"
            )
        )
        logger.info(f"‚úì Model registered as version {model.version}")
        
        # Create environment
        env = ml_client.environments.create_or_update(
            Environment(
                name="pytorch-mnist-inference-v3",
                description="PyTorch environment for MNIST classification",
                conda_file="environment.yml",
                image="mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest"
            )
        )
        logger.info(f"‚úì Environment created: {env.name}:{env.version}")
        
        # Delete existing endpoint if it exists
        try:
            ml_client.online_endpoints.begin_delete(name=ENDPOINT_NAME).result()
            logger.info("‚úì Deleted existing endpoint")
            time.sleep(20)
        except ResourceNotFoundError:
            logger.info("‚ÑπÔ∏è  No existing endpoint found")
        
        # Create endpoint
        endpoint = ManagedOnlineEndpoint(
            name=ENDPOINT_NAME,
            auth_mode="key",
            description="MNIST digit classification endpoint"
        )
        
        endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()
        logger.info("‚úì Endpoint created")
        
        # Create deployment
        deployment = ManagedOnlineDeployment(
            name=DEPLOYMENT_NAME,
            endpoint_name=ENDPOINT_NAME,
            model=model,
            environment=env,
            code_configuration=CodeConfiguration(code=".", scoring_script="score.py"),
            instance_type="Standard_DS3_v2",
            instance_count=1
        )
        
        ml_client.online_deployments.begin_create_or_update(deployment).result()
        logger.info("‚úì Deployment created")
        
        # Set traffic to 100% using create_or_update (v2 SDK)
        endpoint.traffic = {DEPLOYMENT_NAME: 100}
        ml_client.online_endpoints.begin_create_or_update(endpoint).result()
        logger.info("‚úì Traffic routing configured")
        
        # Get endpoint details
        endpoint = ml_client.online_endpoints.get(ENDPOINT_NAME)
        logger.info(f"üéâ DEPLOYMENT SUCCESSFUL!")
        logger.info(f"    Endpoint: {endpoint.name}")
        logger.info(f"    Status: {endpoint.provisioning_state}")
        logger.info(f"    Scoring URI: {endpoint.scoring_uri}")
        
    except Exception as e:
        logger.error(f"‚ùå Azure ML deployment failed: {e}")
        logger.info("üí° Check Azure ML workspace configuration and quotas")

def main():
    """Main pipeline execution"""
    logger.info("üöÄ STARTING COMPLETE MODEL PIPELINE")
    logger.info("=" * 60)
    
    start_time = time.time()
    
    try:
        # 1. Environment setup
        torch, nn, optim, datasets, transforms, DataLoader, quantization, prune, jit, onnx = setup_environment()
        
        # 2. Load data
        train_loader, test_loader = load_mnist_data(datasets, transforms, DataLoader)
        
        # 3. Train models
        sgd_model = train_model('SGD', train_loader, nn, optim, torch)
        sgd_accuracy = evaluate_model(sgd_model, test_loader, torch)
        
        adam_model = train_model('Adam', train_loader, nn, optim, torch)
        adam_accuracy = evaluate_model(adam_model, test_loader, torch)
        
        logger.info(f"üìä SGD Accuracy: {sgd_accuracy:.2f}%")
        logger.info(f"üìä Adam Accuracy: {adam_accuracy:.2f}%")
        
        # Use the better performing model
        best_model = adam_model if adam_accuracy > sgd_accuracy else sgd_model
        best_optimizer = 'Adam' if adam_accuracy > sgd_accuracy else 'SGD'
        best_accuracy = max(adam_accuracy, sgd_accuracy)
        
        logger.info(f"üèÜ Best model: {best_optimizer} with {best_accuracy:.2f}% accuracy")
        
        # 4. Save and load models
        save_models(best_model, torch)
        loaded_model_state, loaded_model_complete = load_models(torch, nn)
        
        # 5. Model optimization
        quantized_model = quantize_model(loaded_model_state, torch, quantization)
        pruned_model = prune_model(loaded_model_state, prune, nn, torch)
        
        # 6. Model conversion
        torchscript_model = convert_to_torchscript(loaded_model_state, torch, jit)
        convert_to_onnx(loaded_model_state, torch)
        
        # 7. Model versioning
        version_manager = ModelVersionManager()
        training_metadata = {
            'dataset': 'MNIST',
            'epochs': 3,
            'accuracy': best_accuracy / 100,
            'optimizer': best_optimizer,
            'learning_rate': 0.001 if best_optimizer == 'Adam' else 0.01
        }
        
        version_name, version_dir = version_manager.save_versioned_model(
            loaded_model_state, 
            'SimpleNN_MNIST', 
            training_metadata
        )
        
        versions = version_manager.list_versions('SimpleNN_MNIST')
        logger.info(f"üìã Total model versions available: {len(versions)}")
        
        # 8. Azure ML deployment (optional)
        ml_client = setup_azure_ml()
        if ml_client:
            register_model_azure(ml_client)
            create_deployment_files()
            deploy_to_azure(ml_client)
        
        # Pipeline completion
        end_time = time.time()
        duration = end_time - start_time
        
        logger.info("=" * 60)
        logger.info("üéâ PIPELINE COMPLETED SUCCESSFULLY!")
        logger.info(f"‚è±Ô∏è  Total execution time: {duration:.2f} seconds")
        logger.info("=" * 60)
        
        # Summary
        logger.info("üìã PIPELINE SUMMARY:")
        logger.info(f"  ‚úì Best model accuracy: {best_accuracy:.2f}%")
        logger.info(f"  ‚úì Models saved in: models/")
        logger.info(f"  ‚úì Version saved: {version_name}")
        logger.info(f"  ‚úì TorchScript model: models/simple_nn_traced.pt")
        logger.info(f"  ‚úì ONNX model: models/simple_nn.onnx")
        if ml_client:
            logger.info(f"  ‚úì Azure ML deployment: attempted")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"‚ùå PIPELINE FAILED: {e}")
        logger.error(f"‚ùå Full traceback: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()
